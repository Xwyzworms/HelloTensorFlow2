{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "GPU name \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(\"GPU name {}\".format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence modelling \n",
    "\n",
    "## Coding tutorials\n",
    " #### [1.  The IMDb dataset](#coding_tutorial_1)\n",
    " #### [2. Padding and masking sequence data](#coding_tutorial_2)\n",
    " #### [3. The Embedding layer](#coding_tutorial_3)\n",
    " #### [4. The Embedding Projector](#coding_tutorial_4)\n",
    " #### [5. Recurrent neural network layers](#coding_tutorial_5)\n",
    " #### [6. Stacked RNNs and the Bidirectional wrapper](#coding_tutorial_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## The IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imdb\n",
    "\n",
    "import tensorflow.keras.datasets.imdb as imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download and assign the data set using load_data()\n",
    "\n",
    "(x_train, y_train) ,(x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Inspect the type of the data\n",
    "\n",
    "\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape of the data\n",
    "\n",
    "print(x_train.shape,y_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first dataset element input\n",
    "# Notice encoding\n",
    "x_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first dataset element output\n",
    "y_train[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with defaults\n",
    "imdb.load_data(path=\"imdb.npz\",index_from=3) # Top Vocab ambil yang ke 4 , 1 + 3 \n",
    "\n",
    "# ~/.keras/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "imdb.load_data(num_words = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the top 10 most frequent words using skip_top\n",
    "imdb.load_data(skip_top=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the sequence lengths to 500 using maxlen\n",
    " \n",
    "imdb.load_data(maxlen=500) # only 500 words per sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "imdb.load_data(start_char=3) # 3 will appear in every dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb word index using get_word_index()\n",
    "\n",
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34704,\n",
       " 'tsukino': 52009,\n",
       " 'nunnery': 52010,\n",
       " 'sonja': 16819,\n",
       " 'vani': 63954,\n",
       " 'woods': 1411,\n",
       " 'spiders': 16118,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'trawling': 52011,\n",
       " \"hold's\": 52012,\n",
       " 'comically': 11310,\n",
       " 'localized': 40833,\n",
       " 'disobeying': 30571,\n",
       " \"'royale\": 52013,\n",
       " \"harpo's\": 40834,\n",
       " 'canet': 52014,\n",
       " 'aileen': 19316,\n",
       " 'acurately': 52015,\n",
       " \"diplomat's\": 52016,\n",
       " 'rickman': 25245,\n",
       " 'arranged': 6749,\n",
       " 'rumbustious': 52017,\n",
       " 'familiarness': 52018,\n",
       " \"spider'\": 52019,\n",
       " 'hahahah': 68807,\n",
       " \"wood'\": 52020,\n",
       " 'transvestism': 40836,\n",
       " \"hangin'\": 34705,\n",
       " 'bringing': 2341,\n",
       " 'seamier': 40837,\n",
       " 'wooded': 34706,\n",
       " 'bravora': 52021,\n",
       " 'grueling': 16820,\n",
       " 'wooden': 1639,\n",
       " 'wednesday': 16821,\n",
       " \"'prix\": 52022,\n",
       " 'altagracia': 34707,\n",
       " 'circuitry': 52023,\n",
       " 'crotch': 11588,\n",
       " 'busybody': 57769,\n",
       " \"tart'n'tangy\": 52024,\n",
       " 'burgade': 14132,\n",
       " 'thrace': 52026,\n",
       " \"tom's\": 11041,\n",
       " 'snuggles': 52028,\n",
       " 'francesco': 29117,\n",
       " 'complainers': 52030,\n",
       " 'templarios': 52128,\n",
       " '272': 40838,\n",
       " '273': 52031,\n",
       " 'zaniacs': 52133,\n",
       " '275': 34709,\n",
       " 'consenting': 27634,\n",
       " 'snuggled': 40839,\n",
       " 'inanimate': 15495,\n",
       " 'uality': 52033,\n",
       " 'bronte': 11929,\n",
       " 'errors': 4013,\n",
       " 'dialogs': 3233,\n",
       " \"yomada's\": 52034,\n",
       " \"madman's\": 34710,\n",
       " 'dialoge': 30588,\n",
       " 'usenet': 52036,\n",
       " 'videodrome': 40840,\n",
       " \"kid'\": 26341,\n",
       " 'pawed': 52037,\n",
       " \"'girlfriend'\": 30572,\n",
       " \"'pleasure\": 52038,\n",
       " \"'reloaded'\": 52039,\n",
       " \"kazakos'\": 40842,\n",
       " 'rocque': 52040,\n",
       " 'mailings': 52041,\n",
       " 'brainwashed': 11930,\n",
       " 'mcanally': 16822,\n",
       " \"tom''\": 52042,\n",
       " 'kurupt': 25246,\n",
       " 'affiliated': 21908,\n",
       " 'babaganoosh': 52043,\n",
       " \"noe's\": 40843,\n",
       " 'quart': 40844,\n",
       " 'kids': 362,\n",
       " 'uplifting': 5037,\n",
       " 'controversy': 7096,\n",
       " 'kida': 21909,\n",
       " 'kidd': 23382,\n",
       " \"error'\": 52044,\n",
       " 'neurologist': 52045,\n",
       " 'spotty': 18513,\n",
       " 'cobblers': 30573,\n",
       " 'projection': 9881,\n",
       " 'fastforwarding': 40845,\n",
       " 'sters': 52046,\n",
       " \"eggar's\": 52047,\n",
       " 'etherything': 52048,\n",
       " 'gateshead': 40846,\n",
       " 'airball': 34711,\n",
       " 'unsinkable': 25247,\n",
       " 'stern': 7183,\n",
       " \"cervi's\": 52049,\n",
       " 'dnd': 40847,\n",
       " 'dna': 11589,\n",
       " 'insecurity': 20601,\n",
       " \"'reboot'\": 52050,\n",
       " 'trelkovsky': 11040,\n",
       " 'jaekel': 52051,\n",
       " 'sidebars': 52052,\n",
       " \"sforza's\": 52053,\n",
       " 'distortions': 17636,\n",
       " 'mutinies': 52054,\n",
       " 'sermons': 30605,\n",
       " '7ft': 40849,\n",
       " 'boobage': 52055,\n",
       " \"o'bannon's\": 52056,\n",
       " 'populations': 23383,\n",
       " 'chulak': 52057,\n",
       " 'mesmerize': 27636,\n",
       " 'quinnell': 52058,\n",
       " 'yahoo': 10310,\n",
       " 'meteorologist': 52060,\n",
       " 'beswick': 42580,\n",
       " 'boorman': 15496,\n",
       " 'voicework': 40850,\n",
       " \"ster'\": 52061,\n",
       " 'blustering': 22925,\n",
       " 'hj': 52062,\n",
       " 'intake': 27637,\n",
       " 'morally': 5624,\n",
       " 'jumbling': 40852,\n",
       " 'bowersock': 52063,\n",
       " \"'porky's'\": 52064,\n",
       " 'gershon': 16824,\n",
       " 'ludicrosity': 40853,\n",
       " 'coprophilia': 52065,\n",
       " 'expressively': 40854,\n",
       " \"india's\": 19503,\n",
       " \"post's\": 34713,\n",
       " 'wana': 52066,\n",
       " 'wang': 5286,\n",
       " 'wand': 30574,\n",
       " 'wane': 25248,\n",
       " 'edgeways': 52324,\n",
       " 'titanium': 34714,\n",
       " 'pinta': 40855,\n",
       " 'want': 181,\n",
       " 'pinto': 30575,\n",
       " 'whoopdedoodles': 52068,\n",
       " 'tchaikovsky': 21911,\n",
       " 'travel': 2106,\n",
       " \"'victory'\": 52069,\n",
       " 'copious': 11931,\n",
       " 'gouge': 22436,\n",
       " \"chapters'\": 52070,\n",
       " 'barbra': 6705,\n",
       " 'uselessness': 30576,\n",
       " \"wan'\": 52071,\n",
       " 'assimilated': 27638,\n",
       " 'petiot': 16119,\n",
       " 'most\\x85and': 52072,\n",
       " 'dinosaurs': 3933,\n",
       " 'wrong': 355,\n",
       " 'seda': 52073,\n",
       " 'stollen': 52074,\n",
       " 'sentencing': 34715,\n",
       " 'ouroboros': 40856,\n",
       " 'assimilates': 40857,\n",
       " 'colorfully': 40858,\n",
       " 'glenne': 27639,\n",
       " 'dongen': 52075,\n",
       " 'subplots': 4763,\n",
       " 'kiloton': 52076,\n",
       " 'chandon': 23384,\n",
       " \"effect'\": 34716,\n",
       " 'snugly': 27640,\n",
       " 'kuei': 40859,\n",
       " 'welcomed': 9095,\n",
       " 'dishonor': 30074,\n",
       " 'concurrence': 52078,\n",
       " 'stoicism': 23385,\n",
       " \"guys'\": 14899,\n",
       " \"beroemd'\": 52080,\n",
       " 'butcher': 6706,\n",
       " \"melfi's\": 40860,\n",
       " 'aargh': 30626,\n",
       " 'playhouse': 20602,\n",
       " 'wickedly': 11311,\n",
       " 'fit': 1183,\n",
       " 'labratory': 52081,\n",
       " 'lifeline': 40862,\n",
       " 'screaming': 1930,\n",
       " 'fix': 4290,\n",
       " 'cineliterate': 52082,\n",
       " 'fic': 52083,\n",
       " 'fia': 52084,\n",
       " 'fig': 34717,\n",
       " 'fmvs': 52085,\n",
       " 'fie': 52086,\n",
       " 'reentered': 52087,\n",
       " 'fin': 30577,\n",
       " 'doctresses': 52088,\n",
       " 'fil': 52089,\n",
       " 'zucker': 12609,\n",
       " 'ached': 31934,\n",
       " 'counsil': 52091,\n",
       " 'paterfamilias': 52092,\n",
       " 'songwriter': 13888,\n",
       " 'shivam': 34718,\n",
       " 'hurting': 9657,\n",
       " 'effects': 302,\n",
       " 'slauther': 52093,\n",
       " \"'flame'\": 52094,\n",
       " 'sommerset': 52095,\n",
       " 'interwhined': 52096,\n",
       " 'whacking': 27641,\n",
       " 'bartok': 52097,\n",
       " 'barton': 8778,\n",
       " 'frewer': 21912,\n",
       " \"fi'\": 52098,\n",
       " 'ingrid': 6195,\n",
       " 'stribor': 30578,\n",
       " 'approporiately': 52099,\n",
       " 'wobblyhand': 52100,\n",
       " 'tantalisingly': 52101,\n",
       " 'ankylosaurus': 52102,\n",
       " 'parasites': 17637,\n",
       " 'childen': 52103,\n",
       " \"jenkins'\": 52104,\n",
       " 'metafiction': 52105,\n",
       " 'golem': 17638,\n",
       " 'indiscretion': 40863,\n",
       " \"reeves'\": 23386,\n",
       " \"inamorata's\": 57784,\n",
       " 'brittannica': 52107,\n",
       " 'adapt': 7919,\n",
       " \"russo's\": 30579,\n",
       " 'guitarists': 48249,\n",
       " 'abbott': 10556,\n",
       " 'abbots': 40864,\n",
       " 'lanisha': 17652,\n",
       " 'magickal': 40866,\n",
       " 'mattter': 52108,\n",
       " \"'willy\": 52109,\n",
       " 'pumpkins': 34719,\n",
       " 'stuntpeople': 52110,\n",
       " 'estimate': 30580,\n",
       " 'ugghhh': 40867,\n",
       " 'gameplay': 11312,\n",
       " \"wern't\": 52111,\n",
       " \"n'sync\": 40868,\n",
       " 'sickeningly': 16120,\n",
       " 'chiara': 40869,\n",
       " 'disturbed': 4014,\n",
       " 'portmanteau': 40870,\n",
       " 'ineffectively': 52112,\n",
       " \"duchonvey's\": 82146,\n",
       " \"nasty'\": 37522,\n",
       " 'purpose': 1288,\n",
       " 'lazers': 52115,\n",
       " 'lightened': 28108,\n",
       " 'kaliganj': 52116,\n",
       " 'popularism': 52117,\n",
       " \"damme's\": 18514,\n",
       " 'stylistics': 30581,\n",
       " 'mindgaming': 52118,\n",
       " 'spoilerish': 46452,\n",
       " \"'corny'\": 52120,\n",
       " 'boerner': 34721,\n",
       " 'olds': 6795,\n",
       " 'bakelite': 52121,\n",
       " 'renovated': 27642,\n",
       " 'forrester': 27643,\n",
       " \"lumiere's\": 52122,\n",
       " 'gaskets': 52027,\n",
       " 'needed': 887,\n",
       " 'smight': 34722,\n",
       " 'master': 1300,\n",
       " \"edie's\": 25908,\n",
       " 'seeber': 40871,\n",
       " 'hiya': 52123,\n",
       " 'fuzziness': 52124,\n",
       " 'genesis': 14900,\n",
       " 'rewards': 12610,\n",
       " 'enthrall': 30582,\n",
       " \"'about\": 40872,\n",
       " \"recollection's\": 52125,\n",
       " 'mutilated': 11042,\n",
       " 'fatherlands': 52126,\n",
       " \"fischer's\": 52127,\n",
       " 'positively': 5402,\n",
       " '270': 34708,\n",
       " 'ahmed': 34723,\n",
       " 'zatoichi': 9839,\n",
       " 'bannister': 13889,\n",
       " 'anniversaries': 52130,\n",
       " \"helm's\": 30583,\n",
       " \"'work'\": 52131,\n",
       " 'exclaimed': 34724,\n",
       " \"'unfunny'\": 52132,\n",
       " '274': 52032,\n",
       " 'feeling': 547,\n",
       " \"wanda's\": 52134,\n",
       " 'dolan': 33269,\n",
       " '278': 52136,\n",
       " 'peacoat': 52137,\n",
       " 'brawny': 40873,\n",
       " 'mishra': 40874,\n",
       " 'worlders': 40875,\n",
       " 'protags': 52138,\n",
       " 'skullcap': 52139,\n",
       " 'dastagir': 57599,\n",
       " 'affairs': 5625,\n",
       " 'wholesome': 7802,\n",
       " 'hymen': 52140,\n",
       " 'paramedics': 25249,\n",
       " 'unpersons': 52141,\n",
       " 'heavyarms': 52142,\n",
       " 'affaire': 52143,\n",
       " 'coulisses': 52144,\n",
       " 'hymer': 40876,\n",
       " 'kremlin': 52145,\n",
       " 'shipments': 30584,\n",
       " 'pixilated': 52146,\n",
       " \"'00s\": 30585,\n",
       " 'diminishing': 18515,\n",
       " 'cinematic': 1360,\n",
       " 'resonates': 14901,\n",
       " 'simplify': 40877,\n",
       " \"nature'\": 40878,\n",
       " 'temptresses': 40879,\n",
       " 'reverence': 16825,\n",
       " 'resonated': 19505,\n",
       " 'dailey': 34725,\n",
       " '2\\x85': 52147,\n",
       " 'treize': 27644,\n",
       " 'majo': 52148,\n",
       " 'kiya': 21913,\n",
       " 'woolnough': 52149,\n",
       " 'thanatos': 39800,\n",
       " 'sandoval': 35734,\n",
       " 'dorama': 40882,\n",
       " \"o'shaughnessy\": 52150,\n",
       " 'tech': 4991,\n",
       " 'fugitives': 32021,\n",
       " 'teck': 30586,\n",
       " \"'e'\": 76128,\n",
       " 'doesn’t': 40884,\n",
       " 'purged': 52152,\n",
       " 'saying': 660,\n",
       " \"martians'\": 41098,\n",
       " 'norliss': 23421,\n",
       " 'dickey': 27645,\n",
       " 'dicker': 52155,\n",
       " \"'sependipity\": 52156,\n",
       " 'padded': 8425,\n",
       " 'ordell': 57795,\n",
       " \"sturges'\": 40885,\n",
       " 'independentcritics': 52157,\n",
       " 'tempted': 5748,\n",
       " \"atkinson's\": 34727,\n",
       " 'hounded': 25250,\n",
       " 'apace': 52158,\n",
       " 'clicked': 15497,\n",
       " \"'humor'\": 30587,\n",
       " \"martino's\": 17180,\n",
       " \"'supporting\": 52159,\n",
       " 'warmongering': 52035,\n",
       " \"zemeckis's\": 34728,\n",
       " 'lube': 21914,\n",
       " 'shocky': 52160,\n",
       " 'plate': 7479,\n",
       " 'plata': 40886,\n",
       " 'sturgess': 40887,\n",
       " \"nerds'\": 40888,\n",
       " 'plato': 20603,\n",
       " 'plath': 34729,\n",
       " 'platt': 40889,\n",
       " 'mcnab': 52162,\n",
       " 'clumsiness': 27646,\n",
       " 'altogether': 3902,\n",
       " 'massacring': 42587,\n",
       " 'bicenntinial': 52163,\n",
       " 'skaal': 40890,\n",
       " 'droning': 14363,\n",
       " 'lds': 8779,\n",
       " 'jaguar': 21915,\n",
       " \"cale's\": 34730,\n",
       " 'nicely': 1780,\n",
       " 'mummy': 4591,\n",
       " \"lot's\": 18516,\n",
       " 'patch': 10089,\n",
       " 'kerkhof': 50205,\n",
       " \"leader's\": 52164,\n",
       " \"'movie\": 27647,\n",
       " 'uncomfirmed': 52165,\n",
       " 'heirloom': 40891,\n",
       " 'wrangle': 47363,\n",
       " 'emotion\\x85': 52166,\n",
       " \"'stargate'\": 52167,\n",
       " 'pinoy': 40892,\n",
       " 'conchatta': 40893,\n",
       " 'broeke': 41131,\n",
       " 'advisedly': 40894,\n",
       " \"barker's\": 17639,\n",
       " 'descours': 52169,\n",
       " 'lots': 775,\n",
       " 'lotr': 9262,\n",
       " 'irs': 9882,\n",
       " 'lott': 52170,\n",
       " 'xvi': 40895,\n",
       " 'irk': 34731,\n",
       " 'irl': 52171,\n",
       " 'ira': 6890,\n",
       " 'belzer': 21916,\n",
       " 'irc': 52172,\n",
       " 'ire': 27648,\n",
       " 'requisites': 40896,\n",
       " 'discipline': 7696,\n",
       " 'lyoko': 52964,\n",
       " 'extend': 11313,\n",
       " 'nature': 876,\n",
       " \"'dickie'\": 52173,\n",
       " 'optimist': 40897,\n",
       " 'lapping': 30589,\n",
       " 'superficial': 3903,\n",
       " 'vestment': 52174,\n",
       " 'extent': 2826,\n",
       " 'tendons': 52175,\n",
       " \"heller's\": 52176,\n",
       " 'quagmires': 52177,\n",
       " 'miyako': 52178,\n",
       " 'moocow': 20604,\n",
       " \"coles'\": 52179,\n",
       " 'lookit': 40898,\n",
       " 'ravenously': 52180,\n",
       " 'levitating': 40899,\n",
       " 'perfunctorily': 52181,\n",
       " 'lookin': 30590,\n",
       " \"lot'\": 40901,\n",
       " 'lookie': 52182,\n",
       " 'fearlessly': 34873,\n",
       " 'libyan': 52184,\n",
       " 'fondles': 40902,\n",
       " 'gopher': 35717,\n",
       " 'wearying': 40904,\n",
       " \"nz's\": 52185,\n",
       " 'minuses': 27649,\n",
       " 'puposelessly': 52186,\n",
       " 'shandling': 52187,\n",
       " 'decapitates': 31271,\n",
       " 'humming': 11932,\n",
       " \"'nother\": 40905,\n",
       " 'smackdown': 21917,\n",
       " 'underdone': 30591,\n",
       " 'frf': 40906,\n",
       " 'triviality': 52188,\n",
       " 'fro': 25251,\n",
       " 'bothers': 8780,\n",
       " \"'kensington\": 52189,\n",
       " 'much': 76,\n",
       " 'muco': 34733,\n",
       " 'wiseguy': 22618,\n",
       " \"richie's\": 27651,\n",
       " 'tonino': 40907,\n",
       " 'unleavened': 52190,\n",
       " 'fry': 11590,\n",
       " \"'tv'\": 40908,\n",
       " 'toning': 40909,\n",
       " 'obese': 14364,\n",
       " 'sensationalized': 30592,\n",
       " 'spiv': 40910,\n",
       " 'spit': 6262,\n",
       " 'arkin': 7367,\n",
       " 'charleton': 21918,\n",
       " 'jeon': 16826,\n",
       " 'boardroom': 21919,\n",
       " 'doubts': 4992,\n",
       " 'spin': 3087,\n",
       " 'hepo': 53086,\n",
       " 'wildcat': 27652,\n",
       " 'venoms': 10587,\n",
       " 'misconstrues': 52194,\n",
       " 'mesmerising': 18517,\n",
       " 'misconstrued': 40911,\n",
       " 'rescinds': 52195,\n",
       " 'prostrate': 52196,\n",
       " 'majid': 40912,\n",
       " 'climbed': 16482,\n",
       " 'canoeing': 34734,\n",
       " 'majin': 52198,\n",
       " 'animie': 57807,\n",
       " 'sylke': 40913,\n",
       " 'conditioned': 14902,\n",
       " 'waddell': 40914,\n",
       " '3\\x85': 52199,\n",
       " 'hyperdrive': 41191,\n",
       " 'conditioner': 34735,\n",
       " 'bricklayer': 53156,\n",
       " 'hong': 2579,\n",
       " 'memoriam': 52201,\n",
       " 'inventively': 30595,\n",
       " \"levant's\": 25252,\n",
       " 'portobello': 20641,\n",
       " 'remand': 52203,\n",
       " 'mummified': 19507,\n",
       " 'honk': 27653,\n",
       " 'spews': 19508,\n",
       " 'visitations': 40915,\n",
       " 'mummifies': 52204,\n",
       " 'cavanaugh': 25253,\n",
       " 'zeon': 23388,\n",
       " \"jungle's\": 40916,\n",
       " 'viertel': 34736,\n",
       " 'frenchmen': 27654,\n",
       " 'torpedoes': 52205,\n",
       " 'schlessinger': 52206,\n",
       " 'torpedoed': 34737,\n",
       " 'blister': 69879,\n",
       " 'cinefest': 52207,\n",
       " 'furlough': 34738,\n",
       " 'mainsequence': 52208,\n",
       " 'mentors': 40917,\n",
       " 'academic': 9097,\n",
       " 'stillness': 20605,\n",
       " 'academia': 40918,\n",
       " 'lonelier': 52209,\n",
       " 'nibby': 52210,\n",
       " \"losers'\": 52211,\n",
       " 'cineastes': 40919,\n",
       " 'corporate': 4452,\n",
       " 'massaging': 40920,\n",
       " 'bellow': 30596,\n",
       " 'absurdities': 19509,\n",
       " 'expetations': 53244,\n",
       " 'nyfiken': 40921,\n",
       " 'mehras': 75641,\n",
       " 'lasse': 52212,\n",
       " 'visability': 52213,\n",
       " 'militarily': 33949,\n",
       " \"elder'\": 52214,\n",
       " 'gainsbourg': 19026,\n",
       " 'hah': 20606,\n",
       " 'hai': 13423,\n",
       " 'haj': 34739,\n",
       " 'hak': 25254,\n",
       " 'hal': 4314,\n",
       " 'ham': 4895,\n",
       " 'duffer': 53262,\n",
       " 'haa': 52216,\n",
       " 'had': 69,\n",
       " 'advancement': 11933,\n",
       " 'hag': 16828,\n",
       " \"hand'\": 25255,\n",
       " 'hay': 13424,\n",
       " 'mcnamara': 20607,\n",
       " \"mozart's\": 52217,\n",
       " 'duffel': 30734,\n",
       " 'haq': 30597,\n",
       " 'har': 13890,\n",
       " 'has': 47,\n",
       " 'hat': 2404,\n",
       " 'hav': 40922,\n",
       " 'haw': 30598,\n",
       " 'figtings': 52218,\n",
       " 'elders': 15498,\n",
       " 'underpanted': 52219,\n",
       " 'pninson': 52220,\n",
       " 'unequivocally': 27655,\n",
       " \"barbara's\": 23676,\n",
       " \"bello'\": 52222,\n",
       " 'indicative': 13000,\n",
       " 'yawnfest': 40923,\n",
       " 'hexploitation': 52223,\n",
       " \"loder's\": 52224,\n",
       " 'sleuthing': 27656,\n",
       " \"justin's\": 32625,\n",
       " \"'ball\": 52225,\n",
       " \"'summer\": 52226,\n",
       " \"'demons'\": 34938,\n",
       " \"mormon's\": 52228,\n",
       " \"laughton's\": 34740,\n",
       " 'debell': 52229,\n",
       " 'shipyard': 39727,\n",
       " 'unabashedly': 30600,\n",
       " 'disks': 40404,\n",
       " 'crowd': 2293,\n",
       " 'crowe': 10090,\n",
       " \"vancouver's\": 56437,\n",
       " 'mosques': 34741,\n",
       " 'crown': 6630,\n",
       " 'culpas': 52230,\n",
       " 'crows': 27657,\n",
       " 'surrell': 53347,\n",
       " 'flowless': 52232,\n",
       " 'sheirk': 52233,\n",
       " \"'three\": 40926,\n",
       " \"peterson'\": 52234,\n",
       " 'ooverall': 52235,\n",
       " 'perchance': 40927,\n",
       " 'bottom': 1324,\n",
       " 'chabert': 53366,\n",
       " 'sneha': 52236,\n",
       " 'inhuman': 13891,\n",
       " 'ichii': 52237,\n",
       " 'ursla': 52238,\n",
       " 'completly': 30601,\n",
       " 'moviedom': 40928,\n",
       " 'raddick': 52239,\n",
       " 'brundage': 51998,\n",
       " 'brigades': 40929,\n",
       " 'starring': 1184,\n",
       " \"'goal'\": 52240,\n",
       " 'caskets': 52241,\n",
       " 'willcock': 52242,\n",
       " \"threesome's\": 52243,\n",
       " \"mosque'\": 52244,\n",
       " \"cover's\": 52245,\n",
       " 'spaceships': 17640,\n",
       " 'anomalous': 40930,\n",
       " 'ptsd': 27658,\n",
       " 'shirdan': 52246,\n",
       " 'obscenity': 21965,\n",
       " 'lemmings': 30602,\n",
       " 'duccio': 30603,\n",
       " \"levene's\": 52247,\n",
       " \"'gorby'\": 52248,\n",
       " \"teenager's\": 25258,\n",
       " 'marshall': 5343,\n",
       " 'honeymoon': 9098,\n",
       " 'shoots': 3234,\n",
       " 'despised': 12261,\n",
       " 'okabasho': 52249,\n",
       " 'fabric': 8292,\n",
       " 'cannavale': 18518,\n",
       " 'raped': 3540,\n",
       " \"tutt's\": 52250,\n",
       " 'grasping': 17641,\n",
       " 'despises': 18519,\n",
       " \"thief's\": 40931,\n",
       " 'rapes': 8929,\n",
       " 'raper': 52251,\n",
       " \"eyre'\": 27659,\n",
       " 'walchek': 52252,\n",
       " \"elmo's\": 23389,\n",
       " 'perfumes': 40932,\n",
       " 'spurting': 21921,\n",
       " \"exposition'\\x85\": 52253,\n",
       " 'denoting': 52254,\n",
       " 'thesaurus': 34743,\n",
       " \"shoot'\": 40933,\n",
       " 'bonejack': 49762,\n",
       " 'simpsonian': 52256,\n",
       " 'hebetude': 30604,\n",
       " \"hallow's\": 34744,\n",
       " 'desperation\\x85': 52257,\n",
       " 'incinerator': 34745,\n",
       " 'congratulations': 10311,\n",
       " 'humbled': 52258,\n",
       " \"else's\": 5927,\n",
       " 'trelkovski': 40848,\n",
       " \"rape'\": 52259,\n",
       " \"'chapters'\": 59389,\n",
       " '1600s': 52260,\n",
       " 'martian': 7256,\n",
       " 'nicest': 25259,\n",
       " 'eyred': 52262,\n",
       " 'passenger': 9460,\n",
       " 'disgrace': 6044,\n",
       " 'moderne': 52263,\n",
       " 'barrymore': 5123,\n",
       " 'yankovich': 52264,\n",
       " 'moderns': 40934,\n",
       " 'studliest': 52265,\n",
       " 'bedsheet': 52266,\n",
       " 'decapitation': 14903,\n",
       " 'slurring': 52267,\n",
       " \"'nunsploitation'\": 52268,\n",
       " \"'character'\": 34746,\n",
       " 'cambodia': 9883,\n",
       " 'rebelious': 52269,\n",
       " 'pasadena': 27660,\n",
       " 'crowne': 40935,\n",
       " \"'bedchamber\": 52270,\n",
       " 'conjectural': 52271,\n",
       " 'appologize': 52272,\n",
       " 'halfassing': 52273,\n",
       " 'paycheque': 57819,\n",
       " 'palms': 20609,\n",
       " \"'islands\": 52274,\n",
       " 'hawked': 40936,\n",
       " 'palme': 21922,\n",
       " 'conservatively': 40937,\n",
       " 'larp': 64010,\n",
       " 'palma': 5561,\n",
       " 'smelling': 21923,\n",
       " 'aragorn': 13001,\n",
       " 'hawker': 52275,\n",
       " 'hawkes': 52276,\n",
       " 'explosions': 3978,\n",
       " 'loren': 8062,\n",
       " \"pyle's\": 52277,\n",
       " 'shootout': 6707,\n",
       " \"mike's\": 18520,\n",
       " \"driscoll's\": 52278,\n",
       " 'cogsworth': 40938,\n",
       " \"britian's\": 52279,\n",
       " 'childs': 34747,\n",
       " \"portrait's\": 52280,\n",
       " 'chain': 3629,\n",
       " 'whoever': 2500,\n",
       " 'puttered': 52281,\n",
       " 'childe': 52282,\n",
       " 'maywether': 52283,\n",
       " 'chair': 3039,\n",
       " \"rance's\": 52284,\n",
       " 'machu': 34748,\n",
       " 'ballet': 4520,\n",
       " 'grapples': 34749,\n",
       " 'summerize': 76155,\n",
       " 'freelance': 30606,\n",
       " \"andrea's\": 52286,\n",
       " '\\x91very': 52287,\n",
       " 'coolidge': 45882,\n",
       " 'mache': 18521,\n",
       " 'balled': 52288,\n",
       " 'grappled': 40940,\n",
       " 'macha': 18522,\n",
       " 'underlining': 21924,\n",
       " 'macho': 5626,\n",
       " 'oversight': 19510,\n",
       " 'machi': 25260,\n",
       " 'verbally': 11314,\n",
       " 'tenacious': 21925,\n",
       " 'windshields': 40941,\n",
       " 'paychecks': 18560,\n",
       " 'jerk': 3399,\n",
       " \"good'\": 11934,\n",
       " 'prancer': 34751,\n",
       " 'prances': 21926,\n",
       " 'olympus': 52289,\n",
       " 'lark': 21927,\n",
       " 'embark': 10788,\n",
       " 'gloomy': 7368,\n",
       " 'jehaan': 52290,\n",
       " 'turaqui': 52291,\n",
       " \"child'\": 20610,\n",
       " 'locked': 2897,\n",
       " 'pranced': 52292,\n",
       " 'exact': 2591,\n",
       " 'unattuned': 52293,\n",
       " 'minute': 786,\n",
       " 'skewed': 16121,\n",
       " 'hodgins': 40943,\n",
       " 'skewer': 34752,\n",
       " 'think\\x85': 52294,\n",
       " 'rosenstein': 38768,\n",
       " 'helmit': 52295,\n",
       " 'wrestlemanias': 34753,\n",
       " 'hindered': 16829,\n",
       " \"martha's\": 30607,\n",
       " 'cheree': 52296,\n",
       " \"pluckin'\": 52297,\n",
       " 'ogles': 40944,\n",
       " 'heavyweight': 11935,\n",
       " 'aada': 82193,\n",
       " 'chopping': 11315,\n",
       " 'strongboy': 61537,\n",
       " 'hegemonic': 41345,\n",
       " 'adorns': 40945,\n",
       " 'xxth': 41349,\n",
       " 'nobuhiro': 34754,\n",
       " 'capitães': 52301,\n",
       " 'kavogianni': 52302,\n",
       " 'antwerp': 13425,\n",
       " 'celebrated': 6541,\n",
       " 'roarke': 52303,\n",
       " 'baggins': 40946,\n",
       " 'cheeseburgers': 31273,\n",
       " 'matras': 52304,\n",
       " \"nineties'\": 52305,\n",
       " \"'craig'\": 52306,\n",
       " 'celebrates': 13002,\n",
       " 'unintentionally': 3386,\n",
       " 'drafted': 14365,\n",
       " 'climby': 52307,\n",
       " '303': 52308,\n",
       " 'oldies': 18523,\n",
       " 'climbs': 9099,\n",
       " 'honour': 9658,\n",
       " 'plucking': 34755,\n",
       " '305': 30077,\n",
       " 'address': 5517,\n",
       " 'menjou': 40947,\n",
       " \"'freak'\": 42595,\n",
       " 'dwindling': 19511,\n",
       " 'benson': 9461,\n",
       " 'white’s': 52310,\n",
       " 'shamelessness': 40948,\n",
       " 'impacted': 21928,\n",
       " 'upatz': 52311,\n",
       " 'cusack': 3843,\n",
       " \"flavia's\": 37570,\n",
       " 'effette': 52312,\n",
       " 'influx': 34756,\n",
       " 'boooooooo': 52313,\n",
       " 'dimitrova': 52314,\n",
       " 'houseman': 13426,\n",
       " 'bigas': 25262,\n",
       " 'boylen': 52315,\n",
       " 'phillipenes': 52316,\n",
       " 'fakery': 40949,\n",
       " \"grandpa's\": 27661,\n",
       " 'darnell': 27662,\n",
       " 'undergone': 19512,\n",
       " 'handbags': 52318,\n",
       " 'perished': 21929,\n",
       " 'pooped': 37781,\n",
       " 'vigour': 27663,\n",
       " 'opposed': 3630,\n",
       " 'etude': 52319,\n",
       " \"caine's\": 11802,\n",
       " 'doozers': 52320,\n",
       " 'photojournals': 34757,\n",
       " 'perishes': 52321,\n",
       " 'constrains': 34758,\n",
       " 'migenes': 40951,\n",
       " 'consoled': 30608,\n",
       " 'alastair': 16830,\n",
       " 'wvs': 52322,\n",
       " 'ooooooh': 52323,\n",
       " 'approving': 34759,\n",
       " 'consoles': 40952,\n",
       " 'disparagement': 52067,\n",
       " 'futureistic': 52325,\n",
       " 'rebounding': 52326,\n",
       " \"'date\": 52327,\n",
       " 'gregoire': 52328,\n",
       " 'rutherford': 21930,\n",
       " 'americanised': 34760,\n",
       " 'novikov': 82199,\n",
       " 'following': 1045,\n",
       " 'munroe': 34761,\n",
       " \"morita'\": 52329,\n",
       " 'christenssen': 52330,\n",
       " 'oatmeal': 23109,\n",
       " 'fossey': 25263,\n",
       " 'livered': 40953,\n",
       " 'listens': 13003,\n",
       " \"'marci\": 76167,\n",
       " \"otis's\": 52333,\n",
       " 'thanking': 23390,\n",
       " 'maude': 16022,\n",
       " 'extensions': 34762,\n",
       " 'ameteurish': 52335,\n",
       " \"commender's\": 52336,\n",
       " 'agricultural': 27664,\n",
       " 'convincingly': 4521,\n",
       " 'fueled': 17642,\n",
       " 'mahattan': 54017,\n",
       " \"paris's\": 40955,\n",
       " 'vulkan': 52339,\n",
       " 'stapes': 52340,\n",
       " 'odysessy': 52341,\n",
       " 'harmon': 12262,\n",
       " 'surfing': 4255,\n",
       " 'halloran': 23497,\n",
       " 'unbelieveably': 49583,\n",
       " \"'offed'\": 52342,\n",
       " 'quadrant': 30610,\n",
       " 'inhabiting': 19513,\n",
       " 'nebbish': 34763,\n",
       " 'forebears': 40956,\n",
       " 'skirmish': 34764,\n",
       " 'ocassionally': 52343,\n",
       " \"'resist\": 52344,\n",
       " 'impactful': 21931,\n",
       " 'spicier': 52345,\n",
       " 'touristy': 40957,\n",
       " \"'football'\": 52346,\n",
       " 'webpage': 40958,\n",
       " 'exurbia': 52348,\n",
       " 'jucier': 52349,\n",
       " 'professors': 14904,\n",
       " 'structuring': 34765,\n",
       " 'jig': 30611,\n",
       " 'overlord': 40959,\n",
       " 'disconnect': 25264,\n",
       " 'sniffle': 82204,\n",
       " 'slimeball': 40960,\n",
       " 'jia': 40961,\n",
       " 'milked': 16831,\n",
       " 'banjoes': 40962,\n",
       " 'jim': 1240,\n",
       " 'workforces': 52351,\n",
       " 'jip': 52352,\n",
       " 'rotweiller': 52353,\n",
       " 'mundaneness': 34766,\n",
       " \"'ninja'\": 52354,\n",
       " \"dead'\": 11043,\n",
       " \"cipriani's\": 40963,\n",
       " 'modestly': 20611,\n",
       " \"professor'\": 52355,\n",
       " 'shacked': 40964,\n",
       " 'bashful': 34767,\n",
       " 'sorter': 23391,\n",
       " 'overpowering': 16123,\n",
       " 'workmanlike': 18524,\n",
       " 'henpecked': 27665,\n",
       " 'sorted': 18525,\n",
       " \"jōb's\": 52357,\n",
       " \"'always\": 52358,\n",
       " \"'baptists\": 34768,\n",
       " 'dreamcatchers': 52359,\n",
       " \"'silence'\": 52360,\n",
       " 'hickory': 21932,\n",
       " 'fun\\x97yet': 52361,\n",
       " 'breakumentary': 52362,\n",
       " 'didn': 15499,\n",
       " 'didi': 52363,\n",
       " 'pealing': 52364,\n",
       " 'dispite': 40965,\n",
       " \"italy's\": 25265,\n",
       " 'instability': 21933,\n",
       " 'quarter': 6542,\n",
       " 'quartet': 12611,\n",
       " 'padmé': 52365,\n",
       " \"'bleedmedry\": 52366,\n",
       " 'pahalniuk': 52367,\n",
       " 'honduras': 52368,\n",
       " 'bursting': 10789,\n",
       " \"pablo's\": 41468,\n",
       " 'irremediably': 52370,\n",
       " 'presages': 40966,\n",
       " 'bowlegged': 57835,\n",
       " 'dalip': 65186,\n",
       " 'entering': 6263,\n",
       " 'newsradio': 76175,\n",
       " 'presaged': 54153,\n",
       " \"giallo's\": 27666,\n",
       " 'bouyant': 40967,\n",
       " 'amerterish': 52371,\n",
       " 'rajni': 18526,\n",
       " 'leeves': 30613,\n",
       " 'macauley': 34770,\n",
       " 'seriously': 615,\n",
       " 'sugercoma': 52372,\n",
       " 'grimstead': 52373,\n",
       " \"'fairy'\": 52374,\n",
       " 'zenda': 30614,\n",
       " \"'twins'\": 52375,\n",
       " 'realisation': 17643,\n",
       " 'highsmith': 27667,\n",
       " 'raunchy': 7820,\n",
       " 'incentives': 40968,\n",
       " 'flatson': 52377,\n",
       " 'snooker': 35100,\n",
       " 'crazies': 16832,\n",
       " 'crazier': 14905,\n",
       " 'grandma': 7097,\n",
       " 'napunsaktha': 52378,\n",
       " 'workmanship': 30615,\n",
       " 'reisner': 52379,\n",
       " \"sanford's\": 61309,\n",
       " '\\x91doña': 52380,\n",
       " 'modest': 6111,\n",
       " \"everything's\": 19156,\n",
       " 'hamer': 40969,\n",
       " \"couldn't'\": 52382,\n",
       " 'quibble': 13004,\n",
       " 'socking': 52383,\n",
       " 'tingler': 21934,\n",
       " 'gutman': 52384,\n",
       " 'lachlan': 40970,\n",
       " 'tableaus': 52385,\n",
       " 'headbanger': 52386,\n",
       " 'spoken': 2850,\n",
       " 'cerebrally': 34771,\n",
       " \"'road\": 23493,\n",
       " 'tableaux': 21935,\n",
       " \"proust's\": 40971,\n",
       " 'periodical': 40972,\n",
       " \"shoveller's\": 52388,\n",
       " 'tamara': 25266,\n",
       " 'affords': 17644,\n",
       " 'concert': 3252,\n",
       " \"yara's\": 87958,\n",
       " 'someome': 52389,\n",
       " 'lingering': 8427,\n",
       " \"abraham's\": 41514,\n",
       " 'beesley': 34772,\n",
       " 'cherbourg': 34773,\n",
       " 'kagan': 28627,\n",
       " 'snatch': 9100,\n",
       " \"miyazaki's\": 9263,\n",
       " 'absorbs': 25267,\n",
       " \"koltai's\": 40973,\n",
       " 'tingled': 64030,\n",
       " 'crossroads': 19514,\n",
       " 'rehab': 16124,\n",
       " 'falworth': 52392,\n",
       " 'sequals': 52393,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the word index as a dictionary,\n",
    "# accounting for index_from.\n",
    "\n",
    "index_from = 3\n",
    "imdb_word_index = {key: value + index_from for key, value in imdb_word_index.items()}\n",
    "imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'film',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'casting',\n",
       " 'location',\n",
       " 'scenery',\n",
       " 'story',\n",
       " 'direction',\n",
       " \"everyone's\",\n",
       " 'really',\n",
       " 'suited',\n",
       " 'the',\n",
       " 'part',\n",
       " 'they',\n",
       " 'played',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'just',\n",
       " 'imagine',\n",
       " 'being',\n",
       " 'there',\n",
       " 'robert',\n",
       " \"redford's\",\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'actor',\n",
       " 'and',\n",
       " 'now',\n",
       " 'the',\n",
       " 'same',\n",
       " 'being',\n",
       " 'director',\n",
       " \"norman's\",\n",
       " 'father',\n",
       " 'came',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'scottish',\n",
       " 'island',\n",
       " 'as',\n",
       " 'myself',\n",
       " 'so',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'real',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'this',\n",
       " 'film',\n",
       " 'the',\n",
       " 'witty',\n",
       " 'remarks',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'film',\n",
       " 'were',\n",
       " 'great',\n",
       " 'it',\n",
       " 'was',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'so',\n",
       " 'much',\n",
       " 'that',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'released',\n",
       " 'for',\n",
       " 'retail',\n",
       " 'and',\n",
       " 'would',\n",
       " 'recommend',\n",
       " 'it',\n",
       " 'to',\n",
       " 'everyone',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fly',\n",
       " 'fishing',\n",
       " 'was',\n",
       " 'amazing',\n",
       " 'really',\n",
       " 'cried',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'was',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'and',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'say',\n",
       " 'if',\n",
       " 'you',\n",
       " 'cry',\n",
       " 'at',\n",
       " 'a',\n",
       " 'film',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'been',\n",
       " 'good',\n",
       " 'and',\n",
       " 'this',\n",
       " 'definitely',\n",
       " 'was',\n",
       " 'also',\n",
       " 'congratulations',\n",
       " 'to',\n",
       " 'the',\n",
       " 'two',\n",
       " 'little',\n",
       " \"boy's\",\n",
       " 'that',\n",
       " 'played',\n",
       " 'the',\n",
       " \"part's\",\n",
       " 'of',\n",
       " 'norman',\n",
       " 'and',\n",
       " 'paul',\n",
       " 'they',\n",
       " 'were',\n",
       " 'just',\n",
       " 'brilliant',\n",
       " 'children',\n",
       " 'are',\n",
       " 'often',\n",
       " 'left',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'praising',\n",
       " 'list',\n",
       " 'i',\n",
       " 'think',\n",
       " 'because',\n",
       " 'the',\n",
       " 'stars',\n",
       " 'that',\n",
       " 'play',\n",
       " 'them',\n",
       " 'all',\n",
       " 'grown',\n",
       " 'up',\n",
       " 'are',\n",
       " 'such',\n",
       " 'a',\n",
       " 'big',\n",
       " 'profile',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'film',\n",
       " 'but',\n",
       " 'these',\n",
       " 'children',\n",
       " 'are',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'praised',\n",
       " 'for',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'you',\n",
       " 'think',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'story',\n",
       " 'was',\n",
       " 'so',\n",
       " 'lovely',\n",
       " 'because',\n",
       " 'it',\n",
       " 'was',\n",
       " 'true',\n",
       " 'and',\n",
       " 'was',\n",
       " \"someone's\",\n",
       " 'life',\n",
       " 'after',\n",
       " 'all',\n",
       " 'that',\n",
       " 'was',\n",
       " 'shared',\n",
       " 'with',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a specific word's index\n",
    "Inverted_dict_index = {value:key for key,value in imdb_word_index.items()} \n",
    "[Inverted_dict_index[key] for key in x_train[0] if key > index_from]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'hamiltons',\n",
       " 'tells',\n",
       " 'the',\n",
       " 'story',\n",
       " 'of',\n",
       " 'the',\n",
       " 'four',\n",
       " 'hamilton',\n",
       " 'siblings',\n",
       " 'teenager',\n",
       " 'francis',\n",
       " 'cory',\n",
       " 'knauf',\n",
       " 'twins',\n",
       " 'wendell',\n",
       " 'joseph',\n",
       " 'mckelheer',\n",
       " 'darlene',\n",
       " 'mackenzie',\n",
       " 'firgens',\n",
       " 'the',\n",
       " 'eldest',\n",
       " 'david',\n",
       " 'samuel',\n",
       " 'who',\n",
       " 'is',\n",
       " 'now',\n",
       " 'the',\n",
       " 'surrogate',\n",
       " 'parent',\n",
       " 'in',\n",
       " 'charge',\n",
       " 'the',\n",
       " \"hamilton's\",\n",
       " 'move',\n",
       " 'house',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'franics',\n",
       " 'is',\n",
       " 'unsure',\n",
       " 'why',\n",
       " 'is',\n",
       " 'unhappy',\n",
       " 'with',\n",
       " 'the',\n",
       " 'way',\n",
       " 'things',\n",
       " 'are',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'his',\n",
       " \"brother's\",\n",
       " 'sister',\n",
       " 'kidnap',\n",
       " 'imprison',\n",
       " 'murder',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'basement',\n",
       " \"doesn't\",\n",
       " 'help',\n",
       " 'relax',\n",
       " 'or',\n",
       " 'calm',\n",
       " \"francis'\",\n",
       " 'nerves',\n",
       " 'either',\n",
       " 'francis',\n",
       " \"know's\",\n",
       " 'something',\n",
       " 'just',\n",
       " \"isn't\",\n",
       " 'right',\n",
       " 'when',\n",
       " 'he',\n",
       " 'eventually',\n",
       " 'finds',\n",
       " 'out',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'things',\n",
       " 'will',\n",
       " 'never',\n",
       " 'be',\n",
       " 'the',\n",
       " 'same',\n",
       " 'again',\n",
       " 'br',\n",
       " 'br',\n",
       " 'co',\n",
       " 'written',\n",
       " 'co',\n",
       " 'produced',\n",
       " 'directed',\n",
       " 'by',\n",
       " 'mitchell',\n",
       " 'altieri',\n",
       " 'phil',\n",
       " 'flores',\n",
       " 'as',\n",
       " 'the',\n",
       " 'butcher',\n",
       " 'brothers',\n",
       " \"who's\",\n",
       " 'only',\n",
       " 'other',\n",
       " 'film',\n",
       " \"director's\",\n",
       " 'credit',\n",
       " 'so',\n",
       " 'far',\n",
       " 'is',\n",
       " 'the',\n",
       " 'april',\n",
       " \"fool's\",\n",
       " 'day',\n",
       " '2008',\n",
       " 'remake',\n",
       " 'enough',\n",
       " 'said',\n",
       " 'this',\n",
       " 'was',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " \"'films\",\n",
       " 'to',\n",
       " 'die',\n",
       " \"for'\",\n",
       " 'at',\n",
       " 'the',\n",
       " '2006',\n",
       " 'after',\n",
       " 'dark',\n",
       " 'horrorfest',\n",
       " 'or',\n",
       " 'whatever',\n",
       " \"it's\",\n",
       " 'called',\n",
       " 'in',\n",
       " 'keeping',\n",
       " 'with',\n",
       " 'pretty',\n",
       " 'much',\n",
       " 'all',\n",
       " 'the',\n",
       " \"other's\",\n",
       " \"i've\",\n",
       " 'seen',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'the',\n",
       " 'hamiltons',\n",
       " 'was',\n",
       " 'complete',\n",
       " 'total',\n",
       " 'utter',\n",
       " 'crap',\n",
       " 'i',\n",
       " 'found',\n",
       " 'the',\n",
       " \"character's\",\n",
       " 'really',\n",
       " 'poor',\n",
       " 'very',\n",
       " 'unlikable',\n",
       " 'the',\n",
       " 'slow',\n",
       " 'moving',\n",
       " 'story',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'my',\n",
       " 'imagination',\n",
       " 'or',\n",
       " 'sustain',\n",
       " 'my',\n",
       " 'interest',\n",
       " 'over',\n",
       " \"it's\",\n",
       " '85',\n",
       " 'a',\n",
       " 'half',\n",
       " 'minute',\n",
       " 'too',\n",
       " 'long',\n",
       " '86',\n",
       " 'minute',\n",
       " 'duration',\n",
       " 'the',\n",
       " \"there's\",\n",
       " 'the',\n",
       " 'awful',\n",
       " 'twist',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'which',\n",
       " 'had',\n",
       " 'me',\n",
       " 'laughing',\n",
       " 'out',\n",
       " 'loud',\n",
       " \"there's\",\n",
       " 'this',\n",
       " 'really',\n",
       " 'big',\n",
       " 'sustained',\n",
       " 'build',\n",
       " 'up',\n",
       " 'to',\n",
       " \"what's\",\n",
       " 'inside',\n",
       " 'a',\n",
       " 'cupboard',\n",
       " 'thing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hamiltons',\n",
       " 'basement',\n",
       " \"it's\",\n",
       " 'eventually',\n",
       " 'revealed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'little',\n",
       " 'boy',\n",
       " 'with',\n",
       " 'a',\n",
       " 'teddy',\n",
       " 'is',\n",
       " 'that',\n",
       " 'really',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'scare',\n",
       " 'us',\n",
       " 'is',\n",
       " 'that',\n",
       " 'really',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'shock',\n",
       " 'us',\n",
       " 'is',\n",
       " 'that',\n",
       " 'really',\n",
       " 'something',\n",
       " 'that',\n",
       " 'is',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'have',\n",
       " 'us',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'it',\n",
       " 'as',\n",
       " 'the',\n",
       " 'end',\n",
       " 'credits',\n",
       " 'roll',\n",
       " 'is',\n",
       " 'a',\n",
       " 'harmless',\n",
       " 'looking',\n",
       " 'young',\n",
       " 'boy',\n",
       " 'the',\n",
       " 'best',\n",
       " \"'twist'\",\n",
       " 'ending',\n",
       " 'that',\n",
       " 'the',\n",
       " 'makers',\n",
       " 'could',\n",
       " 'come',\n",
       " 'up',\n",
       " 'with',\n",
       " 'the',\n",
       " 'boring',\n",
       " 'plot',\n",
       " 'plods',\n",
       " 'along',\n",
       " \"it's\",\n",
       " 'never',\n",
       " 'made',\n",
       " 'clear',\n",
       " 'where',\n",
       " 'the',\n",
       " 'hamiltons',\n",
       " 'get',\n",
       " 'all',\n",
       " 'their',\n",
       " 'money',\n",
       " 'from',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'new',\n",
       " 'houses',\n",
       " 'since',\n",
       " 'none',\n",
       " 'of',\n",
       " 'them',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'work',\n",
       " 'except',\n",
       " 'david',\n",
       " 'in',\n",
       " 'a',\n",
       " 'slaughterhouse',\n",
       " 'i',\n",
       " 'doubt',\n",
       " 'that',\n",
       " 'pays',\n",
       " 'much',\n",
       " 'or',\n",
       " 'why',\n",
       " 'they',\n",
       " \"haven't\",\n",
       " 'been',\n",
       " 'caught',\n",
       " 'before',\n",
       " 'now',\n",
       " 'the',\n",
       " 'script',\n",
       " 'tries',\n",
       " 'to',\n",
       " 'mix',\n",
       " 'in',\n",
       " 'every',\n",
       " 'day',\n",
       " 'drama',\n",
       " 'with',\n",
       " 'potent',\n",
       " 'horror',\n",
       " 'it',\n",
       " 'just',\n",
       " 'does',\n",
       " 'a',\n",
       " 'terrible',\n",
       " 'job',\n",
       " 'of',\n",
       " 'combining',\n",
       " 'the',\n",
       " 'two',\n",
       " 'to',\n",
       " 'the',\n",
       " 'extent',\n",
       " 'that',\n",
       " 'neither',\n",
       " 'aspect',\n",
       " 'is',\n",
       " 'memorable',\n",
       " 'or',\n",
       " 'effective',\n",
       " 'a',\n",
       " 'really',\n",
       " 'bad',\n",
       " 'film',\n",
       " 'that',\n",
       " 'i',\n",
       " 'am',\n",
       " 'struggling',\n",
       " 'to',\n",
       " 'say',\n",
       " 'anything',\n",
       " 'good',\n",
       " 'about',\n",
       " 'br',\n",
       " 'br',\n",
       " 'despite',\n",
       " 'being',\n",
       " 'written',\n",
       " 'directed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'extreme',\n",
       " 'sounding',\n",
       " 'butcher',\n",
       " 'brothers',\n",
       " \"there's\",\n",
       " 'no',\n",
       " 'gore',\n",
       " 'here',\n",
       " \"there's\",\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'blood',\n",
       " 'splatter',\n",
       " 'a',\n",
       " 'few',\n",
       " 'scenes',\n",
       " 'of',\n",
       " 'girls',\n",
       " 'chained',\n",
       " 'up',\n",
       " 'in',\n",
       " 'a',\n",
       " 'basement',\n",
       " 'but',\n",
       " 'nothing',\n",
       " 'you',\n",
       " \"couldn't\",\n",
       " 'do',\n",
       " 'at',\n",
       " 'home',\n",
       " 'yourself',\n",
       " 'with',\n",
       " 'a',\n",
       " 'bottle',\n",
       " 'of',\n",
       " 'tomato',\n",
       " 'ketchup',\n",
       " 'a',\n",
       " 'camcorder',\n",
       " 'the',\n",
       " 'film',\n",
       " 'is',\n",
       " 'neither',\n",
       " 'scary',\n",
       " 'since',\n",
       " \"it's\",\n",
       " 'got',\n",
       " 'a',\n",
       " 'very',\n",
       " 'middle',\n",
       " 'class',\n",
       " 'suburban',\n",
       " 'setting',\n",
       " \"there's\",\n",
       " 'zero',\n",
       " 'atmosphere',\n",
       " 'or',\n",
       " 'mood',\n",
       " \"there's\",\n",
       " 'a',\n",
       " 'lesbian',\n",
       " 'suggest',\n",
       " 'incestuous',\n",
       " 'kiss',\n",
       " 'but',\n",
       " 'the',\n",
       " 'hamiltons',\n",
       " 'is',\n",
       " 'low',\n",
       " 'on',\n",
       " 'the',\n",
       " 'exploitation',\n",
       " 'scale',\n",
       " \"there's\",\n",
       " 'not',\n",
       " 'much',\n",
       " 'here',\n",
       " 'for',\n",
       " 'the',\n",
       " 'horror',\n",
       " 'crowd',\n",
       " 'br',\n",
       " 'br',\n",
       " 'filmed',\n",
       " 'in',\n",
       " 'petaluma',\n",
       " 'in',\n",
       " 'california',\n",
       " 'this',\n",
       " 'has',\n",
       " 'that',\n",
       " 'modern',\n",
       " 'low',\n",
       " 'budget',\n",
       " 'look',\n",
       " 'about',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'not',\n",
       " 'badly',\n",
       " 'made',\n",
       " 'but',\n",
       " 'rather',\n",
       " 'forgettable',\n",
       " 'the',\n",
       " 'acting',\n",
       " 'by',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'to',\n",
       " 'me',\n",
       " 'cast',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'write',\n",
       " 'home',\n",
       " 'about',\n",
       " 'i',\n",
       " \"can't\",\n",
       " 'say',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'felt',\n",
       " 'anything',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'br',\n",
       " 'br',\n",
       " 'the',\n",
       " 'hamiltons',\n",
       " 'commits',\n",
       " 'the',\n",
       " 'cardinal',\n",
       " 'sin',\n",
       " 'of',\n",
       " 'being',\n",
       " 'both',\n",
       " 'dull',\n",
       " 'boring',\n",
       " 'from',\n",
       " 'which',\n",
       " 'it',\n",
       " 'never',\n",
       " 'recovers',\n",
       " 'add',\n",
       " 'to',\n",
       " 'that',\n",
       " 'an',\n",
       " 'ultra',\n",
       " 'thin',\n",
       " 'story',\n",
       " 'no',\n",
       " 'gore',\n",
       " 'a',\n",
       " 'rubbish',\n",
       " 'ending',\n",
       " \"character's\",\n",
       " 'who',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'give',\n",
       " 'a',\n",
       " 'toss',\n",
       " 'about',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'film',\n",
       " 'that',\n",
       " 'did',\n",
       " 'not',\n",
       " 'impress',\n",
       " 'me',\n",
       " 'at',\n",
       " 'all']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "\n",
    "[Inverted_dict_index[key] for key in x_train[7] if key > index_from]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sentiment value\n",
    "\n",
    "y_train[0] , y_train[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Padding and Masking Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb data set\n",
    "\n",
    "import tensorflow.keras.datasets.imdb as imdb\n",
    "(x_train,y_train) , (x_test,y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the input data shape\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "\n",
    "padded_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen = 300, padding=\"post\", truncating=\"pre\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
       "           65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
       "            5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
       "            9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
       "          167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
       "           17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
       "            6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
       "          469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
       "           38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
       "           17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
       "           12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
       "           16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
       "           38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
       "           25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
       "           52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
       "          107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
       "            5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
       "          317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
       "            4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
       "          141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
       "          134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
       "           51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
       "           65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
       "           16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
       "          178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0], dtype=int32),\n",
       " [1,\n",
       "  14,\n",
       "  22,\n",
       "  16,\n",
       "  43,\n",
       "  530,\n",
       "  973,\n",
       "  1622,\n",
       "  1385,\n",
       "  65,\n",
       "  458,\n",
       "  4468,\n",
       "  66,\n",
       "  3941,\n",
       "  4,\n",
       "  173,\n",
       "  36,\n",
       "  256,\n",
       "  5,\n",
       "  25,\n",
       "  100,\n",
       "  43,\n",
       "  838,\n",
       "  112,\n",
       "  50,\n",
       "  670,\n",
       "  22665,\n",
       "  9,\n",
       "  35,\n",
       "  480,\n",
       "  284,\n",
       "  5,\n",
       "  150,\n",
       "  4,\n",
       "  172,\n",
       "  112,\n",
       "  167,\n",
       "  21631,\n",
       "  336,\n",
       "  385,\n",
       "  39,\n",
       "  4,\n",
       "  172,\n",
       "  4536,\n",
       "  1111,\n",
       "  17,\n",
       "  546,\n",
       "  38,\n",
       "  13,\n",
       "  447,\n",
       "  4,\n",
       "  192,\n",
       "  50,\n",
       "  16,\n",
       "  6,\n",
       "  147,\n",
       "  2025,\n",
       "  19,\n",
       "  14,\n",
       "  22,\n",
       "  4,\n",
       "  1920,\n",
       "  4613,\n",
       "  469,\n",
       "  4,\n",
       "  22,\n",
       "  71,\n",
       "  87,\n",
       "  12,\n",
       "  16,\n",
       "  43,\n",
       "  530,\n",
       "  38,\n",
       "  76,\n",
       "  15,\n",
       "  13,\n",
       "  1247,\n",
       "  4,\n",
       "  22,\n",
       "  17,\n",
       "  515,\n",
       "  17,\n",
       "  12,\n",
       "  16,\n",
       "  626,\n",
       "  18,\n",
       "  19193,\n",
       "  5,\n",
       "  62,\n",
       "  386,\n",
       "  12,\n",
       "  8,\n",
       "  316,\n",
       "  8,\n",
       "  106,\n",
       "  5,\n",
       "  4,\n",
       "  2223,\n",
       "  5244,\n",
       "  16,\n",
       "  480,\n",
       "  66,\n",
       "  3785,\n",
       "  33,\n",
       "  4,\n",
       "  130,\n",
       "  12,\n",
       "  16,\n",
       "  38,\n",
       "  619,\n",
       "  5,\n",
       "  25,\n",
       "  124,\n",
       "  51,\n",
       "  36,\n",
       "  135,\n",
       "  48,\n",
       "  25,\n",
       "  1415,\n",
       "  33,\n",
       "  6,\n",
       "  22,\n",
       "  12,\n",
       "  215,\n",
       "  28,\n",
       "  77,\n",
       "  52,\n",
       "  5,\n",
       "  14,\n",
       "  407,\n",
       "  16,\n",
       "  82,\n",
       "  10311,\n",
       "  8,\n",
       "  4,\n",
       "  107,\n",
       "  117,\n",
       "  5952,\n",
       "  15,\n",
       "  256,\n",
       "  4,\n",
       "  31050,\n",
       "  7,\n",
       "  3766,\n",
       "  5,\n",
       "  723,\n",
       "  36,\n",
       "  71,\n",
       "  43,\n",
       "  530,\n",
       "  476,\n",
       "  26,\n",
       "  400,\n",
       "  317,\n",
       "  46,\n",
       "  7,\n",
       "  4,\n",
       "  12118,\n",
       "  1029,\n",
       "  13,\n",
       "  104,\n",
       "  88,\n",
       "  4,\n",
       "  381,\n",
       "  15,\n",
       "  297,\n",
       "  98,\n",
       "  32,\n",
       "  2071,\n",
       "  56,\n",
       "  26,\n",
       "  141,\n",
       "  6,\n",
       "  194,\n",
       "  7486,\n",
       "  18,\n",
       "  4,\n",
       "  226,\n",
       "  22,\n",
       "  21,\n",
       "  134,\n",
       "  476,\n",
       "  26,\n",
       "  480,\n",
       "  5,\n",
       "  144,\n",
       "  30,\n",
       "  5535,\n",
       "  18,\n",
       "  51,\n",
       "  36,\n",
       "  28,\n",
       "  224,\n",
       "  92,\n",
       "  25,\n",
       "  104,\n",
       "  4,\n",
       "  226,\n",
       "  65,\n",
       "  16,\n",
       "  38,\n",
       "  1334,\n",
       "  88,\n",
       "  12,\n",
       "  16,\n",
       "  283,\n",
       "  5,\n",
       "  16,\n",
       "  4472,\n",
       "  113,\n",
       "  103,\n",
       "  32,\n",
       "  15,\n",
       "  16,\n",
       "  5345,\n",
       "  19,\n",
       "  178,\n",
       "  32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the output data shape\n",
    "\n",
    "padded_x_train[0] , x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   1],\n",
       "        [  14],\n",
       "        [  22],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   1],\n",
       "        [ 194],\n",
       "        [1153],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   1],\n",
       "        [  14],\n",
       "        [  47],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[   1],\n",
       "        [  11],\n",
       "        [   6],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   1],\n",
       "        [1446],\n",
       "        [7079],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]],\n",
       "\n",
       "       [[   1],\n",
       "        [  17],\n",
       "        [   6],\n",
       "        ...,\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0]]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "padded_x_train = np.expand_dims(padded_x_train,axis = -1)\n",
    "padded_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Masking layer \n",
    "\n",
    "tf_x_train = tf.convert_to_tensor(padded_x_train, dtype = tf.float32)\n",
    "masking_layer = tf.keras.layers.Masking(mask_value = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass tf_x_train to it\n",
    "masked_x_train = masking_layer(tf_x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=13, shape=(300, 1), dtype=float32, numpy=\n",
       "array([[1.0000e+00],\n",
       "       [1.4000e+01],\n",
       "       [2.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [4.3000e+01],\n",
       "       [5.3000e+02],\n",
       "       [9.7300e+02],\n",
       "       [1.6220e+03],\n",
       "       [1.3850e+03],\n",
       "       [6.5000e+01],\n",
       "       [4.5800e+02],\n",
       "       [4.4680e+03],\n",
       "       [6.6000e+01],\n",
       "       [3.9410e+03],\n",
       "       [4.0000e+00],\n",
       "       [1.7300e+02],\n",
       "       [3.6000e+01],\n",
       "       [2.5600e+02],\n",
       "       [5.0000e+00],\n",
       "       [2.5000e+01],\n",
       "       [1.0000e+02],\n",
       "       [4.3000e+01],\n",
       "       [8.3800e+02],\n",
       "       [1.1200e+02],\n",
       "       [5.0000e+01],\n",
       "       [6.7000e+02],\n",
       "       [2.2665e+04],\n",
       "       [9.0000e+00],\n",
       "       [3.5000e+01],\n",
       "       [4.8000e+02],\n",
       "       [2.8400e+02],\n",
       "       [5.0000e+00],\n",
       "       [1.5000e+02],\n",
       "       [4.0000e+00],\n",
       "       [1.7200e+02],\n",
       "       [1.1200e+02],\n",
       "       [1.6700e+02],\n",
       "       [2.1631e+04],\n",
       "       [3.3600e+02],\n",
       "       [3.8500e+02],\n",
       "       [3.9000e+01],\n",
       "       [4.0000e+00],\n",
       "       [1.7200e+02],\n",
       "       [4.5360e+03],\n",
       "       [1.1110e+03],\n",
       "       [1.7000e+01],\n",
       "       [5.4600e+02],\n",
       "       [3.8000e+01],\n",
       "       [1.3000e+01],\n",
       "       [4.4700e+02],\n",
       "       [4.0000e+00],\n",
       "       [1.9200e+02],\n",
       "       [5.0000e+01],\n",
       "       [1.6000e+01],\n",
       "       [6.0000e+00],\n",
       "       [1.4700e+02],\n",
       "       [2.0250e+03],\n",
       "       [1.9000e+01],\n",
       "       [1.4000e+01],\n",
       "       [2.2000e+01],\n",
       "       [4.0000e+00],\n",
       "       [1.9200e+03],\n",
       "       [4.6130e+03],\n",
       "       [4.6900e+02],\n",
       "       [4.0000e+00],\n",
       "       [2.2000e+01],\n",
       "       [7.1000e+01],\n",
       "       [8.7000e+01],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [4.3000e+01],\n",
       "       [5.3000e+02],\n",
       "       [3.8000e+01],\n",
       "       [7.6000e+01],\n",
       "       [1.5000e+01],\n",
       "       [1.3000e+01],\n",
       "       [1.2470e+03],\n",
       "       [4.0000e+00],\n",
       "       [2.2000e+01],\n",
       "       [1.7000e+01],\n",
       "       [5.1500e+02],\n",
       "       [1.7000e+01],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [6.2600e+02],\n",
       "       [1.8000e+01],\n",
       "       [1.9193e+04],\n",
       "       [5.0000e+00],\n",
       "       [6.2000e+01],\n",
       "       [3.8600e+02],\n",
       "       [1.2000e+01],\n",
       "       [8.0000e+00],\n",
       "       [3.1600e+02],\n",
       "       [8.0000e+00],\n",
       "       [1.0600e+02],\n",
       "       [5.0000e+00],\n",
       "       [4.0000e+00],\n",
       "       [2.2230e+03],\n",
       "       [5.2440e+03],\n",
       "       [1.6000e+01],\n",
       "       [4.8000e+02],\n",
       "       [6.6000e+01],\n",
       "       [3.7850e+03],\n",
       "       [3.3000e+01],\n",
       "       [4.0000e+00],\n",
       "       [1.3000e+02],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [3.8000e+01],\n",
       "       [6.1900e+02],\n",
       "       [5.0000e+00],\n",
       "       [2.5000e+01],\n",
       "       [1.2400e+02],\n",
       "       [5.1000e+01],\n",
       "       [3.6000e+01],\n",
       "       [1.3500e+02],\n",
       "       [4.8000e+01],\n",
       "       [2.5000e+01],\n",
       "       [1.4150e+03],\n",
       "       [3.3000e+01],\n",
       "       [6.0000e+00],\n",
       "       [2.2000e+01],\n",
       "       [1.2000e+01],\n",
       "       [2.1500e+02],\n",
       "       [2.8000e+01],\n",
       "       [7.7000e+01],\n",
       "       [5.2000e+01],\n",
       "       [5.0000e+00],\n",
       "       [1.4000e+01],\n",
       "       [4.0700e+02],\n",
       "       [1.6000e+01],\n",
       "       [8.2000e+01],\n",
       "       [1.0311e+04],\n",
       "       [8.0000e+00],\n",
       "       [4.0000e+00],\n",
       "       [1.0700e+02],\n",
       "       [1.1700e+02],\n",
       "       [5.9520e+03],\n",
       "       [1.5000e+01],\n",
       "       [2.5600e+02],\n",
       "       [4.0000e+00],\n",
       "       [3.1050e+04],\n",
       "       [7.0000e+00],\n",
       "       [3.7660e+03],\n",
       "       [5.0000e+00],\n",
       "       [7.2300e+02],\n",
       "       [3.6000e+01],\n",
       "       [7.1000e+01],\n",
       "       [4.3000e+01],\n",
       "       [5.3000e+02],\n",
       "       [4.7600e+02],\n",
       "       [2.6000e+01],\n",
       "       [4.0000e+02],\n",
       "       [3.1700e+02],\n",
       "       [4.6000e+01],\n",
       "       [7.0000e+00],\n",
       "       [4.0000e+00],\n",
       "       [1.2118e+04],\n",
       "       [1.0290e+03],\n",
       "       [1.3000e+01],\n",
       "       [1.0400e+02],\n",
       "       [8.8000e+01],\n",
       "       [4.0000e+00],\n",
       "       [3.8100e+02],\n",
       "       [1.5000e+01],\n",
       "       [2.9700e+02],\n",
       "       [9.8000e+01],\n",
       "       [3.2000e+01],\n",
       "       [2.0710e+03],\n",
       "       [5.6000e+01],\n",
       "       [2.6000e+01],\n",
       "       [1.4100e+02],\n",
       "       [6.0000e+00],\n",
       "       [1.9400e+02],\n",
       "       [7.4860e+03],\n",
       "       [1.8000e+01],\n",
       "       [4.0000e+00],\n",
       "       [2.2600e+02],\n",
       "       [2.2000e+01],\n",
       "       [2.1000e+01],\n",
       "       [1.3400e+02],\n",
       "       [4.7600e+02],\n",
       "       [2.6000e+01],\n",
       "       [4.8000e+02],\n",
       "       [5.0000e+00],\n",
       "       [1.4400e+02],\n",
       "       [3.0000e+01],\n",
       "       [5.5350e+03],\n",
       "       [1.8000e+01],\n",
       "       [5.1000e+01],\n",
       "       [3.6000e+01],\n",
       "       [2.8000e+01],\n",
       "       [2.2400e+02],\n",
       "       [9.2000e+01],\n",
       "       [2.5000e+01],\n",
       "       [1.0400e+02],\n",
       "       [4.0000e+00],\n",
       "       [2.2600e+02],\n",
       "       [6.5000e+01],\n",
       "       [1.6000e+01],\n",
       "       [3.8000e+01],\n",
       "       [1.3340e+03],\n",
       "       [8.8000e+01],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [2.8300e+02],\n",
       "       [5.0000e+00],\n",
       "       [1.6000e+01],\n",
       "       [4.4720e+03],\n",
       "       [1.1300e+02],\n",
       "       [1.0300e+02],\n",
       "       [3.2000e+01],\n",
       "       [1.5000e+01],\n",
       "       [1.6000e+01],\n",
       "       [5.3450e+03],\n",
       "       [1.9000e+01],\n",
       "       [1.7800e+02],\n",
       "       [3.2000e+01],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the dataset\n",
    "\n",
    "tf_x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=17, shape=(300, 1), dtype=float32, numpy=\n",
       "array([[1.0000e+00],\n",
       "       [1.4000e+01],\n",
       "       [2.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [4.3000e+01],\n",
       "       [5.3000e+02],\n",
       "       [9.7300e+02],\n",
       "       [1.6220e+03],\n",
       "       [1.3850e+03],\n",
       "       [6.5000e+01],\n",
       "       [4.5800e+02],\n",
       "       [4.4680e+03],\n",
       "       [6.6000e+01],\n",
       "       [3.9410e+03],\n",
       "       [4.0000e+00],\n",
       "       [1.7300e+02],\n",
       "       [3.6000e+01],\n",
       "       [2.5600e+02],\n",
       "       [5.0000e+00],\n",
       "       [2.5000e+01],\n",
       "       [1.0000e+02],\n",
       "       [4.3000e+01],\n",
       "       [8.3800e+02],\n",
       "       [1.1200e+02],\n",
       "       [5.0000e+01],\n",
       "       [6.7000e+02],\n",
       "       [2.2665e+04],\n",
       "       [9.0000e+00],\n",
       "       [3.5000e+01],\n",
       "       [4.8000e+02],\n",
       "       [2.8400e+02],\n",
       "       [5.0000e+00],\n",
       "       [1.5000e+02],\n",
       "       [4.0000e+00],\n",
       "       [1.7200e+02],\n",
       "       [1.1200e+02],\n",
       "       [1.6700e+02],\n",
       "       [2.1631e+04],\n",
       "       [3.3600e+02],\n",
       "       [3.8500e+02],\n",
       "       [3.9000e+01],\n",
       "       [4.0000e+00],\n",
       "       [1.7200e+02],\n",
       "       [4.5360e+03],\n",
       "       [1.1110e+03],\n",
       "       [1.7000e+01],\n",
       "       [5.4600e+02],\n",
       "       [3.8000e+01],\n",
       "       [1.3000e+01],\n",
       "       [4.4700e+02],\n",
       "       [4.0000e+00],\n",
       "       [1.9200e+02],\n",
       "       [5.0000e+01],\n",
       "       [1.6000e+01],\n",
       "       [6.0000e+00],\n",
       "       [1.4700e+02],\n",
       "       [2.0250e+03],\n",
       "       [1.9000e+01],\n",
       "       [1.4000e+01],\n",
       "       [2.2000e+01],\n",
       "       [4.0000e+00],\n",
       "       [1.9200e+03],\n",
       "       [4.6130e+03],\n",
       "       [4.6900e+02],\n",
       "       [4.0000e+00],\n",
       "       [2.2000e+01],\n",
       "       [7.1000e+01],\n",
       "       [8.7000e+01],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [4.3000e+01],\n",
       "       [5.3000e+02],\n",
       "       [3.8000e+01],\n",
       "       [7.6000e+01],\n",
       "       [1.5000e+01],\n",
       "       [1.3000e+01],\n",
       "       [1.2470e+03],\n",
       "       [4.0000e+00],\n",
       "       [2.2000e+01],\n",
       "       [1.7000e+01],\n",
       "       [5.1500e+02],\n",
       "       [1.7000e+01],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [6.2600e+02],\n",
       "       [1.8000e+01],\n",
       "       [1.9193e+04],\n",
       "       [5.0000e+00],\n",
       "       [6.2000e+01],\n",
       "       [3.8600e+02],\n",
       "       [1.2000e+01],\n",
       "       [8.0000e+00],\n",
       "       [3.1600e+02],\n",
       "       [8.0000e+00],\n",
       "       [1.0600e+02],\n",
       "       [5.0000e+00],\n",
       "       [4.0000e+00],\n",
       "       [2.2230e+03],\n",
       "       [5.2440e+03],\n",
       "       [1.6000e+01],\n",
       "       [4.8000e+02],\n",
       "       [6.6000e+01],\n",
       "       [3.7850e+03],\n",
       "       [3.3000e+01],\n",
       "       [4.0000e+00],\n",
       "       [1.3000e+02],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [3.8000e+01],\n",
       "       [6.1900e+02],\n",
       "       [5.0000e+00],\n",
       "       [2.5000e+01],\n",
       "       [1.2400e+02],\n",
       "       [5.1000e+01],\n",
       "       [3.6000e+01],\n",
       "       [1.3500e+02],\n",
       "       [4.8000e+01],\n",
       "       [2.5000e+01],\n",
       "       [1.4150e+03],\n",
       "       [3.3000e+01],\n",
       "       [6.0000e+00],\n",
       "       [2.2000e+01],\n",
       "       [1.2000e+01],\n",
       "       [2.1500e+02],\n",
       "       [2.8000e+01],\n",
       "       [7.7000e+01],\n",
       "       [5.2000e+01],\n",
       "       [5.0000e+00],\n",
       "       [1.4000e+01],\n",
       "       [4.0700e+02],\n",
       "       [1.6000e+01],\n",
       "       [8.2000e+01],\n",
       "       [1.0311e+04],\n",
       "       [8.0000e+00],\n",
       "       [4.0000e+00],\n",
       "       [1.0700e+02],\n",
       "       [1.1700e+02],\n",
       "       [5.9520e+03],\n",
       "       [1.5000e+01],\n",
       "       [2.5600e+02],\n",
       "       [4.0000e+00],\n",
       "       [3.1050e+04],\n",
       "       [7.0000e+00],\n",
       "       [3.7660e+03],\n",
       "       [5.0000e+00],\n",
       "       [7.2300e+02],\n",
       "       [3.6000e+01],\n",
       "       [7.1000e+01],\n",
       "       [4.3000e+01],\n",
       "       [5.3000e+02],\n",
       "       [4.7600e+02],\n",
       "       [2.6000e+01],\n",
       "       [4.0000e+02],\n",
       "       [3.1700e+02],\n",
       "       [4.6000e+01],\n",
       "       [7.0000e+00],\n",
       "       [4.0000e+00],\n",
       "       [1.2118e+04],\n",
       "       [1.0290e+03],\n",
       "       [1.3000e+01],\n",
       "       [1.0400e+02],\n",
       "       [8.8000e+01],\n",
       "       [4.0000e+00],\n",
       "       [3.8100e+02],\n",
       "       [1.5000e+01],\n",
       "       [2.9700e+02],\n",
       "       [9.8000e+01],\n",
       "       [3.2000e+01],\n",
       "       [2.0710e+03],\n",
       "       [5.6000e+01],\n",
       "       [2.6000e+01],\n",
       "       [1.4100e+02],\n",
       "       [6.0000e+00],\n",
       "       [1.9400e+02],\n",
       "       [7.4860e+03],\n",
       "       [1.8000e+01],\n",
       "       [4.0000e+00],\n",
       "       [2.2600e+02],\n",
       "       [2.2000e+01],\n",
       "       [2.1000e+01],\n",
       "       [1.3400e+02],\n",
       "       [4.7600e+02],\n",
       "       [2.6000e+01],\n",
       "       [4.8000e+02],\n",
       "       [5.0000e+00],\n",
       "       [1.4400e+02],\n",
       "       [3.0000e+01],\n",
       "       [5.5350e+03],\n",
       "       [1.8000e+01],\n",
       "       [5.1000e+01],\n",
       "       [3.6000e+01],\n",
       "       [2.8000e+01],\n",
       "       [2.2400e+02],\n",
       "       [9.2000e+01],\n",
       "       [2.5000e+01],\n",
       "       [1.0400e+02],\n",
       "       [4.0000e+00],\n",
       "       [2.2600e+02],\n",
       "       [6.5000e+01],\n",
       "       [1.6000e+01],\n",
       "       [3.8000e+01],\n",
       "       [1.3340e+03],\n",
       "       [8.8000e+01],\n",
       "       [1.2000e+01],\n",
       "       [1.6000e+01],\n",
       "       [2.8300e+02],\n",
       "       [5.0000e+00],\n",
       "       [1.6000e+01],\n",
       "       [4.4720e+03],\n",
       "       [1.1300e+02],\n",
       "       [1.0300e+02],\n",
       "       [3.2000e+01],\n",
       "       [1.5000e+01],\n",
       "       [1.6000e+01],\n",
       "       [5.3450e+03],\n",
       "       [1.9000e+01],\n",
       "       [1.7800e+02],\n",
       "       [3.2000e+01],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00],\n",
       "       [0.0000e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9, shape=(25000, 300), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the ._keras_mask for the dataset\n",
    "\n",
    "masked_x_train._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## The Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and apply an `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "# Expects an input of shape (batch, sequence, feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Embedding layer weights using get_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding for the 14th index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and apply an `Embedding` layer that uses `mask_zero=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer that uses the mask_zero kwarg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## The Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first dataset example sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum token value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model using Sequential:\n",
    "#     1. Embedding layer\n",
    "#     2. GlobalAveragePooling1D\n",
    "#     3. Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API refresher: use the Model to build the same model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a binary cross-entropy loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using .fit(), savng its history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TensorFlow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "import io\n",
    "from os import path\n",
    "\n",
    "out_v = io.open(path.join('data', 'vecs.tsv'), 'w', encoding='utf-8')\n",
    "out_m = io.open(path.join('data', 'meta.tsv'), 'w', encoding='utf-8')\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in word_index.items():\n",
    "    if k != 0:\n",
    "        out_m.write('\\n')\n",
    "        out_v.write('\\n')\n",
    "    \n",
    "    out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    out_m.write(word)\n",
    "    k += 1\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()\n",
    "# beware large collections of embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Recurrent neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and pass an input to a SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleRNN layer and test it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that only the final cell output is returned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index using get_imdb_word_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build the model:\n",
    "# 1. Embedding.\n",
    "# 2. LSTM.\n",
    "# 3. Dense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with binary cross-entropy loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and save its training history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first test data example sentence\n",
    "# (invert the word index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model prediction using model.predict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_6\"></a>\n",
    "## Stacked RNNs and the Bidirectional wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index using get_imdb_word_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build stacked and bidirectional recurrent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value and specify an embedding dimension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build a stacked LSTM model via return_sequences=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build a bidirectional RNN with merge_mode='sum'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, saving its history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
